{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Background\n",
    "\n",
    "This notebook is meant to teach some of the basic functionality of Pandas by using a hydrologic dataset to answer a couple simple questions about the behavior of the Glen Canyon Dam on the Colorado River. A short signal processing example is also included to make use of the dataset and illustrate Fast Fourier Transforms in numpy.\n",
    "\n",
    "The [Colorado River Compact](https://en.wikipedia.org/wiki/Colorado_River_Compact).  mandates there should be 7.5E6 Acre-feet /year of flow at Lee's Ferry to deliver the [Compact](https://www.usbr.gov/lc/region/pao/pdfiles/crcompct.pdf)-mandated from the upper basin to the lower basin. This requirement has been in existence since 1922 and USGS has maintained a [streamgage](https://waterdata.usgs.gov/nwis/uv?09380000) there since 1921. \n",
    "\n",
    "\n",
    " <img width=\"500\" src=\"Glen_Canyon_Dam_Lake_Powell,_Arizona.jpeg\">\n",
    " \n",
    "In 1967, The [Glen Canyon Dam](https://www.usbr.gov/uc/rm/crsp/gc/) was filled (construction took place between 1956 and 1966)  in part to ensure this mandated delivery from the upper basin to the lower basin, as well as to generate electricity.\n",
    "\n",
    "\n",
    "In this notebook, we will use Pandas and Numpy to analyze the streamgage data and explore the behavior of the river with respect to the presence of the dam.\n",
    "\n",
    "  1. Pull the data from USGS webservices\n",
    "  2. Munge the data and convert units to prepare for analysis \n",
    "  3. Examine the data to explore the impact of the dam on meeting the compact and river behavior\n",
    "  4. Using a fast-Fourier transform, analyze the signal properties of flow with the dam in place\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. import the python packages that will be needed for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import detrend as sp_detrend\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pulling the data into a pandas DataFrame\n",
    "\n",
    "We already know the site number for Lee's Ferry is 09380000. Now we need to construct a waterservices URL to use Pandas to download the data. And, of course, there will be necessary data munging and unit conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the URL is just a long string\n",
    "site_num = '09380000'\n",
    "\n",
    "dv_url = 'http://waterservices.usgs.gov/nwis/dv/?format=rdb'\n",
    "dv_url += '&sites={0}'.format(site_num)\n",
    "#dv_url += '&startDT=2010-01-01'\n",
    "dv_url += '&startDT=1880-01-01'\n",
    "#dv_url += '&endDT=2018-01-17'\n",
    "dv_url += '&parameterCd=00060'\n",
    "print(dv_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now, ideally, we could just pull the data directly with Pandas, but it is a little tricky to handle the header\n",
    "### So, we can use `requests` to download a text file first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('reading data for {0}'.format(site_num))\n",
    "dv_file = requests.get(dv_url)\n",
    "\n",
    "with open('{0}.dat'.format(site_num), 'w') as ofp:\n",
    "    for line in dv_file:\n",
    "        ofp.write(line.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now just look at the header lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = open('{0}.dat'.format(site_num), 'r').readlines()\n",
    "for i in np.arange(40):\n",
    "    print (file_data[i].rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this gets tricky with USGS NWIS data because the column names are followed by a code that contains metadata that we don't need. \n",
    "\n",
    "We can first read it in as a MultiIndex header (specifying the header to be rows 0 and 1 after the comment lines). We also need to specify that the separator for the columns in the file is a tab (`'\\t'`). \n",
    "\n",
    "Also, note that we are reading directly from the URL - we could also read from the file we downloaded by passing a filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df = pd.read_csv(dv_url, comment='#', header=[0,1], sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at the `head` - top 5 rows, and see the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we only want the first row of each column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df.columns = [i[0] for i in nwis_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and we have to be sure our dates are interpreted as dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initially, they are `object` type, so we can convert them to dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df.datetime = pd.to_datetime(nwis_df.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we want to use the `datetime` column as the index for the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df.set_index('datetime',drop=True,inplace=True) \n",
    "nwis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and finally,  we can drop the column about quality and the agency code and site number as redundant and rename just the flux data with a more meaningful name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df = nwis_df['236239_00060_00003'].to_frame() # reducing to one column makes this a pandas series, but\n",
    "                                                # to add columns later, we want it to stay as a DataFrame\n",
    "nwis_df.columns=['Q']\n",
    "nwis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Now Data Munging\n",
    "### Note that the data starts on October 1, 1921. This means the start of a USGS concept called \"Water Year\". It would be good to perform calculations based on the water year.\n",
    "First, we can make a couple new columns, one for year, and one for water year.\n",
    "\n",
    "How can we group by water year? Not a very easy Google Kung Fu exercise at first, but what about \"Fiscal Year\"?\n",
    "Google \"Pandas group by fiscal year\"\n",
    "http://stackoverflow.com/questions/26341272/using-groupby-on-pandas-dataframe-to-group-by-financial-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make water year by shifting forward the number of days in Oct., Nov., and Dec.\n",
    "# NOTE --> shifting by months is less precise\n",
    "nwis_df['water_year'] = nwis_df.index.shift(30+31+31,freq='d').year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So now we can add columns with some unit conversions. Recall from the introduction that the Compact is in terms of acre-feet but the metadata from NWIS indicates that we have data in cubic feet per second\n",
    "\n",
    "### units are $\\frac{ft^3}{s}$\n",
    "### So let's convert to cubic feet per day which we can later sum up by water year\n",
    "### $\\frac{1 ft^3}{s} \\times \\frac{60s}{min} \\times \\frac{60min}{hour} \\times \\frac{24hours}{day} \\rightarrow \\frac{ft^3}{day}$\n",
    "\n",
    "### 1 acre-foot = 43559.9 cubic feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df['Q_cfd'] = nwis_df.Q * 60 * 60 * 24\n",
    "nwis_df['Q_af'] = nwis_df.Q_cfd / 43559.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Now we can plot the time series and take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df['Q_af'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we can definitely see some interesting things but might be better to look year-by-year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using `groupby` we can easily create a multipage Ppdf with each water year being a page of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages('wateryear_lees_ferry.pdf') as outpdf:\n",
    "    for cname, cgroup in nwis_df.groupby('water_year'):\n",
    "        print('plotting for water year {}\\r'.format(cname), end=\"\")\n",
    "        plt.figure(figsize=(8,6))\n",
    "        cgroup['Q_af'].plot()\n",
    "        plt.title('Flow in Acre feet per day for water year {}'.format(cname))\n",
    "        outpdf.savefig()\n",
    "        plt.close('all')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open the PDF and check out the patterns. What's going on in the mid-1960s? How about [1983](https://www.simonandschuster.com/books/The-Emerald-Mile/Kevin-Fedarko/9781439159866)? Any big differences in the flow regime before and after the dam?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also aggregate data using `agg` - powerful!\n",
    "### let's aggregate by water year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wateryear = nwis_df.groupby('water_year').agg(['count','mean','sum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any Missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this has a multiple index\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.ylabel(\"Count of days\")\n",
    "wateryear['Q_af','count'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's look at statistics to see if there are any missing days prior to 2021 (the current partial year)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wateryear.loc[wateryear.index<2021].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nice! 25% are leap years (mean is close to 365.25), and no years have less than 365 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's evaluate with respect to the Compact. The compact states that each _year_ 7.5 Million acre feet must pass from the upper basin to the lower at Lee's Ferry. Was that happening prior to the dam being installed in the 1960s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the aggregated (using sum) Acre Feet per year prior to 1963\n",
    "ax = wateryear['Q_af','sum'].loc[wateryear.index<1963].plot(figsize=(8,5))\n",
    "ax.axhline(y=7.5e6, c='orange')\n",
    "ax.set_title('annual acre-feet flow prior to dam')\n",
    "ax.set_ylabel('Q [acre feet] per year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about looking at the entire record\n",
    "ax = wateryear['Q_af','sum'].loc[wateryear.index<2021].plot(figsize=(8,5))\n",
    "ax.axhline(y=7.5e6, c='orange')\n",
    "ax.set_title('annual acre-feet flow for entire record')\n",
    "ax.set_ylabel('Q [acre feet] per year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do some more exploration of the flow over all record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df.Q_af.plot(figsize=(8,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first we can apply functions to `groupby` grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean Q in acre feet per day\n",
    "mean_Q = nwis_df.groupby('water_year').Q_af.mean()\n",
    "# calculate the standard deviation\n",
    "CI = nwis_df.groupby('water_year').Q_af.std()\n",
    "# make a filled plot representing the 95% interval around the mean\n",
    "lower_CI = mean_Q - 2*CI\n",
    "upper_CI = mean_Q + 2*CI\n",
    "ax = mean_Q.plot(style='b.-', figsize=(12,4))\n",
    "plt.fill_between(lower_CI.index,lower_CI,upper_CI, color='b',alpha = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### can accomplish this in one step using `agg` for \"aggregate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_agg = nwis_df.groupby('water_year').Q_af.agg([np.min, np.mean, np.std])\n",
    "\n",
    "# make a function so we can try different aggregation strategies\n",
    "def plot_agg(Q_agg): \n",
    "    mean_Q =Q_agg['mean'].apply(lambda x: np.max((x,0.01))) \n",
    "    lower_CI = mean_Q - 2*Q_agg['std'].apply(lambda x: np.max((x,0.01)))\n",
    "    upper_CI = mean_Q + 2*Q_agg['std'].apply(lambda x: np.max((x,0.01)))\n",
    "    ax = mean_Q.plot(style='b.-', figsize=(12,4))\n",
    "    plt.fill_between(Q_agg.index,lower_CI,upper_CI, color='b',alpha = 0.2)\n",
    "plot_agg(Q_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_agg['std'].apply(lambda x: np.min((x,0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The variability certainly changed after the dam was filled. We can check out just standard deviation as a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df.groupby('water_year').Q_af.std().plot(kind='bar',rot=45, figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how hard to change from annual aggregation to monthly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_agg_month = nwis_df.groupby(nwis_df.index.month).Q_af.agg([np.mean,np.std])\n",
    "plot_agg(Q_agg_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAT? why only 12 monthly values? \n",
    "### We need two levels of grouping --- by year then by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_agg_month = nwis_df.groupby([nwis_df.index.year,nwis_df.index.month]).Q_af.aggregate([np.mean,np.std])\n",
    "Q_agg_month['mean'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. We can use Fast Fourier Transform Analysis to further examine the changes in the behavior of the river after the dam was built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier analysis - so nerdy. so fun. Here's some background\n",
    "https://youtu.be/spUNpyF58BY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we have to detrend the data for `fft` to work properly. care why? read this: https://groups.google.com/forum/#!topic/comp.dsp/kYDZqetr_TU\n",
    "\n",
    "### how do we do this? python is \"batteries included\" https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.detrend.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_df['Q_detrend'] = sp_detrend(nwis_df.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(nwis_df)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = plt.subplot(3,1,1)\n",
    "plt.plot(nwis_df.Q_detrend)\n",
    "plt.title('detrended signal')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(nwis_df.index,nwis_df.Q)\n",
    "plt.title('Raw Signal')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(nwis_df.index,nwis_df.Q-nwis_df.Q_detrend)\n",
    "plt.title('Difference')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and plot the Period Spectrum to see timing of recurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First make a function to perform the FFT and plot the spectrum for signals of various length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_and_plot(df, plot_dominant_periods=4):\n",
    "    N = len(df)\n",
    "    yf = np.fft.fft(df.Q_detrend)\n",
    "    yf = np.abs(yf[:int(N/2)])\n",
    "    # get the right frequency \n",
    "    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fftfreq.html#numpy.fft.fftfreq\n",
    "    d = 1. # day\n",
    "    f = np.fft.fftfreq(N,d)[:int(N/2)]\n",
    "    f[0] = .00001\n",
    "    per = 1./f # days\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    ax = plt.subplot(2,1,1)\n",
    "    plt.plot(per, yf)\n",
    "    plt.xscale('log')\n",
    "\n",
    "    top=np.argsort(yf)[-plot_dominant_periods:]\n",
    "    j=(10-plot_dominant_periods)/10\n",
    "    for i in top:\n",
    "        plt.plot([per[i],per[i]],[0,np.max(yf)],'r:')\n",
    "        plt.text(per[i],j*np.max(yf),'{0:.2f}'.format(per[i]))\n",
    "        j+=0.1\n",
    "\n",
    "    plt.title('Period Spectrum')\n",
    "    plt.grid()\n",
    "    ax.set_xlabel('Period (days)')\n",
    "    plt.xlim([1, 1e4])\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(df.index,df.Q)\n",
    "    plt.title('Raw Signal')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we can look at the whole signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_and_plot(nwis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dominant periods are all right around a year, which corresponds to the spring flood. The fourth most dominant peak is around half a year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about only the years before the dam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_and_plot(nwis_df.loc[nwis_df.index.year<1963])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, the annual and biannual periods dominate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### did things change after the dam filled up in 1963?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_and_plot(nwis_df.loc[nwis_df.index.year>1963], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are more noisy with more unique events that imply very long periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we focus in on the heady days of grunge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_and_plot(nwis_df.loc[(nwis_df.index.year>1990) & (nwis_df.index.year<1996)], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what's up with a dominant period of 7 days?\n",
    "\n",
    "### let's check out a couple weeks of the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "cfreq = 40\n",
    "df = nwis_df.loc[nwis_df.index.year>1993]\n",
    "df.iloc[:cfreq].Q.plot(rot=45, grid=True)\n",
    "plt.xticks(df.iloc[:cfreq].index,df.iloc[:cfreq].index.day_name());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looks like weekday power generation with lover flow on the weekends!\n",
    "\n",
    "### finally, how does recent time look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_and_plot(nwis_df.loc[nwis_df.index.year>2019], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, in recent times, weekly power-generation periods are way more dominant than the former annual signal under natural conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
